{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train a deep learning model\nIn this notebook you will train a deep learning model to classify the descriptions of car components as compliant or non-compliant. \n\nEach document in the supplied training data set is a short text description of the component as documented by an authorized technician. \nThe contents include:\n- Manufacture year of the component (e.g. 1985, 2010)\n- Condition of the component (poor, fair, good, new)\n- Materials used in the component (plastic, carbon fiber, steel, iron)\n\nThe compliance regulations dictate:\n*Any component manufactured before 1995 or in fair or poor condition or made with plastic or iron is out of compliance.*\n\nFor example:\n* Manufactured in 1985 made of steel in fair condition -> **Non-compliant**\n* Good condition carbon fiber component manufactured in 2010 -> **Compliant**\n* Steel component manufactured in 1995 in fair condition -> **Non-Compliant**\n\nThe labels present in this data are 0 for compliant, 1 for non-compliant.\n\nThe challenge with classifying text data is that deep learning models only undertand vectors (e.g., arrays of numbers) and not text. To encode the car component descriptions as vectors, we use an algorithm from Stanford called [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). GloVe provides us pre-trained vectors that we can use to convert a string of text into a vector."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setup\nTo begin, you will need to provide the following information about your Azure Subscription.\n\n**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n\n**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n\nIn the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n\nTo get these values, do the following:\n1. Navigate to the Azure Portal and login with the credentials provided.\n2. From the left hand menu, under Favorites, select `Resource Groups`.\n3. In the list, select the resource group with the name similar to `XXXXX`.\n4. From the Overview tab, capture the desired values.\n\nExecute the following cell by selecting the `>|Run` button in the command bar above."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"6ee947fa-0d77-4915-bf68-4a83a8bec2a4\" # <- needs to be the subscription with the Quick-Starts resource group\n\n#Provide values for the existing Resource Group \nresource_group = \"Quick-Starts-MLOps123\" # <- replace XXXXX with your unique identifier\n\n#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\nworkspace_name = \"quick-starts-ws-mlops123\" # <- replace XXXXX with your unique identifier\nworkspace_region = \"northeurope\" # <- region of your Quick-Starts resource group",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'deep-learning'\nproject_folder = './dl'\ndeployment_folder = './deploy'\ndatasets_folder = './datasets'\n\n# this is the URL to the CSV file containing the car component descriptions\ncardata_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n            'quickstarts/connected-car-data/connected-car_components.csv')\n\ncardata_ds_name = 'connected_car_components'\ncardata_ds_description = 'Connected car components data'\n\n# this is the name of the AML Compute cluster\ncluster_name = \"gpucluster\"\n\nembedding_dim = 100                                        \ntraining_samples = 90000                                 \nvalidation_samples = 5000    \nmax_words = 10000",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create the Azure Machine Learning resources"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Azure Machine Learning SDK provides a comprehensive set of a capabilities that you can use directly within a notebook including:\n- Creating a **Workspace** that acts as the root object to organize all artifacts and resources used by Azure Machine Learning.\n- Creating **Experiments** in your Workspace that capture versions of the trained model along with any desired model performance telemetry. Each time you train a model and evaluate its results, you can capture that run (model and telemetry) within an Experiment.\n- Creating **Compute** resources that can be used to scale out model training, so that while your notebook may be running in a lightweight container in Azure Notebooks, your model training can actually occur on a powerful cluster that can provide large amounts of memory, CPU or GPU. \n- Using **Automated Machine Learning (AutoML)** to automatically train multiple versions of a model using a mix of different ways to prepare the data and different algorithms and hyperparameters (algorithm settings) in search of the model that performs best according to a performance metric that you specify.\n- Packaging a Docker **Image** that contains everything your trained model needs for scoring (prediction) in order to run as a web service.\n- Deploying your Image to either Azure Kubernetes or Azure Container Instances, effectively hosting the **Web Service**.\n\nIn Azure Notebooks, all of the libraries needed for Azure Machine Learning are pre-installed. To use them, you just need to import them. Run the following cell to do so:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import logging\nimport os\nimport json\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nimport pandas as pd\n\nimport azureml.core\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.dataset import Dataset\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.widgets import RunDetails\nfrom azureml.train.dnn import TensorFlow\n\nfrom keras.models import load_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "azureml.core.__version__",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'1.0.72'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create and connect to an Azure Machine Learning Workspace\nRun the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n\n**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# By using the exist_ok param, if the worskpace already exists you get a reference to the existing workspace\n# allowing you to re-run this cell multiple times as desired (which is fairly common in notebooks).\nws = Workspace.create(\n    name = workspace_name,\n    subscription_id = subscription_id,\n    resource_group = resource_group, \n    location = workspace_region,\n    exist_ok = True)\n\nws.write_config()\nprint('Workspace configuration succeeded')",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FKLZYLYSZ to authenticate.\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "WARNING - Failed to authenticate '{'additional_properties': {}, 'id': '/tenants/0f277086-d4e0-4971-bc1a-bbc5df0eb246', 'tenant_id': '0f277086-d4e0-4971-bc1a-bbc5df0eb246'}' due to error 'Get Token request returned http error: 400 and server response: {\"error\":\"interaction_required\",\"error_description\":\"AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-dac1f8f63013'.\\r\\nTrace ID: 7d4eb172-08b8-40ec-a603-f85b4b7e4e00\\r\\nCorrelation ID: 3d614027-f5ea-47aa-80a1-5335eb9611a4\\r\\nTimestamp: 2020-02-22 08:29:12Z\",\"error_codes\":[50076],\"timestamp\":\"2020-02-22 08:29:12Z\",\"trace_id\":\"7d4eb172-08b8-40ec-a603-f85b4b7e4e00\",\"correlation_id\":\"3d614027-f5ea-47aa-80a1-5335eb9611a4\",\"error_uri\":\"https://login.microsoftonline.com/error?code=50076\",\"suberror\":\"basic_action\"}'\nWARNING - Failed to authenticate '{'additional_properties': {}, 'id': '/tenants/12437861-f55d-4e74-8b78-47996c60686a', 'tenant_id': '12437861-f55d-4e74-8b78-47996c60686a'}' due to error 'Get Token request returned http error: 400 and server response: {\"error\":\"interaction_required\",\"error_description\":\"AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-dac1f8f63013'.\\r\\nTrace ID: b225bd2c-1b59-4dee-948f-d123b2583500\\r\\nCorrelation ID: 0cc96ca9-d4b5-4f8e-ab72-d9d95981fa3e\\r\\nTimestamp: 2020-02-22 08:29:16Z\",\"error_codes\":[50076],\"timestamp\":\"2020-02-22 08:29:16Z\",\"trace_id\":\"b225bd2c-1b59-4dee-948f-d123b2583500\",\"correlation_id\":\"0cc96ca9-d4b5-4f8e-ab72-d9d95981fa3e\",\"error_uri\":\"https://login.microsoftonline.com/error?code=50076\",\"suberror\":\"basic_action\"}'\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Interactive authentication successfully completed.\nDeploying StorageAccount with name quickstastorage957e4064b.\nDeploying KeyVault with name quickstakeyvaulte73398e5.\nDeployed KeyVault with name quickstakeyvaulte73398e5. Took 22.26 seconds.\nDeploying AppInsights with name quickstainsightsd56b13cd.\nDeployed AppInsights with name quickstainsightsd56b13cd. Took 28.83 seconds.\nDeployed StorageAccount with name quickstastorage957e4064b. Took 56.92 seconds.\nDeploying Workspace with name quick-starts-ws-mlops123.\nDeployed Workspace with name quick-starts-ws-mlops123. Took 54.48 seconds.\nWorkspace configuration succeeded\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create AML Compute Cluster\nNow you are ready to create the GPU compute cluster. Run the following cell to create a new compute cluster (or retrieve the existing cluster if it already exists). The code below will create a *GPU based* cluster where each node in the cluster is of the size `Standard_NC12`, and the cluster is restricted to use 1 node. This will take couple of minutes to create."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### Create AML CPU based Compute Cluster\n\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing compute target.')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC12',\n                                                           min_nodes=1, max_nodes=1)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)\n\n# Use the 'status' property to get a detailed status for the current AmlCompute. \nprint(compute_target.status.serialize())",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating a new compute target...\nCreating\nSucceeded..................\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 1, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2020-02-22T08:40:36.095000+00:00', 'errors': None, 'creationTime': '2020-02-22T08:40:35.184435+00:00', 'modifiedTime': '2020-02-22T08:41:21.487802+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': ''}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC12'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the Keras Estimator"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "keras_est = TensorFlow(source_directory=project_folder,\n                       compute_target=compute_target,\n                       framework_version='2.0',\n                       entry_script='train.py',\n                       conda_packages=['pandas'],\n                       pip_packages=['azureml-dataprep[pandas]', 'keras==2.3.1'], # just add keras through pip\n                       use_gpu=True)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Remotely train a deep learning model using the Azure ML Compute\nIn the following cells, you will *not* train the model against the data you just downloaded using the resources provided by Azure Notebooks. Instead, you will deploy an Azure ML Compute cluster that will download the data and use a trainings script to train the model. In other words, all of the training will be performed remotely with respect to this notebook. \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create project folder\nif not os.path.exists(project_folder):\n    os.makedirs(project_folder)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the training script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $project_folder/train.py\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport keras\nfrom keras import models \nfrom keras import layers\nfrom keras import optimizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\nfrom azureml.core import Run\nfrom azureml.core.dataset import Dataset\n\ndatasets_folder = './datasets'\n\n# this is the URL to the CSV file containing the GloVe vectors\nglove_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n             'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n             'quickstarts/connected-car-data/glove.6B.100d.txt')\n\nglove_ds_name = 'glove_6B_100d'\nglove_ds_description ='GloVe embeddings 6B 100d'\n\n# this is the URL to the CSV file containing the care component descriptions\ncardata_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n            'quickstarts/connected-car-data/connected-car_components.csv')\n\ncardata_ds_name = 'connected_car_components'\ncardata_ds_description = 'Connected car components data'\n\nembedding_dim = 100                                        \ntraining_samples = 90000                                 \nvalidation_samples = 5000    \nmax_words = 10000\n\nrun = Run.get_context()\nws = run.experiment.workspace\n\nprint(\"Downloading GloVe embeddings...\")\n\ntry:\n    glove_ds = Dataset.get_by_name(workspace=ws, name=glove_ds_name)\n    print('GloVe embeddings dataset already registered.')\nexcept:\n    print('Registering GloVe embeddings dataset...')\n    glove_ds = Dataset.File.from_files(glove_url)\n    glove_ds.register(workspace=ws, name=glove_ds_name, description=glove_ds_description)\n    print('GloVe embeddings dataset successfully registered.')\n    \nfile_paths = glove_ds.download(target_path=datasets_folder, overwrite=True)\nglove_file_path = file_paths[0]\nprint(\"Download complete.\")\n\n\n# Load the car components labeled data\nprint(\"Loading car components data...\")\n\ntry:\n    cardata_ds = Dataset.get_by_name(workspace=ws, name=cardata_ds_name)\n    print('Connected car components dataset already registered.')\nexcept:\n    print('Registering connected car components dataset...')\n    cardata_ds = Dataset.Tabular.from_delimited_files(path=cardata_url)\n    cardata_ds.register(workspace=ws, name=cardata_ds_name, description=cardata_ds_description)\n    print('Connected car components dataset successfully registered.')\n\ncar_components_df = cardata_ds.to_pandas_dataframe()\ncomponents = car_components_df[\"text\"].tolist()\nlabels = car_components_df[\"label\"].tolist()\n\nprint(\"Loading car components data completed.\")\n\n\n# use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text\nprint(\"Tokenizing data...\")    \n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(components)\nsequences = tokenizer.texts_to_sequences(components)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=embedding_dim)\n\nlabels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\nindices = np.arange(data.shape[0])  \nnp.random.seed(100)\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\n\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]\n\nx_test = data[training_samples + validation_samples:]\ny_test = labels[training_samples + validation_samples:]\nprint(\"Tokenizing data complete.\")\n\n# apply the vectors provided by GloVe to create a word embedding matrix\nprint(\"Applying GloVe vectors...\")\n\nembeddings_index = {}\nf = open(glove_file_path)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector    \nprint(\"Applying GloVe vectors compelted.\")\n\n# use Keras to define the structure of the deep neural network   \nprint(\"Creating model structure...\")\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=embedding_dim))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\n# fix the weights for the first layer to those provided by the embedding matrix\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False\nprint(\"Creating model structure completed.\")\n\nopt = optimizers.RMSprop(lr=0.1)\n\nprint(\"Training model...\")\nmodel.compile(optimizer=opt,\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(x_train, y_train,\n                    epochs=3, \n                    batch_size=32,\n                    validation_data=(x_val, y_val))\nprint(\"Training model completed.\")\n\nprint(\"Saving model files...\")\n# create a ./outputs/model folder in the compute target\n# files saved in the \"./outputs\" folder are automatically uploaded into run history\nos.makedirs('./outputs/model', exist_ok=True)\n# save model\nmodel.save('./outputs/model/model.h5')\nprint(\"model saved in ./outputs/model folder\")\nprint(\"Saving model files completed.\")",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing ./dl/train.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Submit the training run"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code pattern to submit a training run to Azure Machine Learning compute targets is always:\n\n- Create an experiment to run.\n- Submit the experiment.\n- Wait for the run to complete."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create the experiment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment = Experiment(ws, experiment_name)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Submit the experiment"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = experiment.submit(keras_est)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Wait for the run to complete by executing the following cell. Note that this process will perform the following:\n- Build and deploy the container to Azure Machine Learning compute (~8 minutes)\n- Execute the training script (~2 minutes)\n\nIf you change only the training script and re-submit, it will run faster the second time because the necessary container is already prepared so the time requried is just that for executing the training script.\n\n*Run the cell below and wait till the **Run Status** is **Completed** before proceeding ahead*"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "RunDetails(run).show()",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06f2cc49131b4361844d6f2bb5e7b7da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/deep-learning/runs/deep-learning_1582361152_031f08ea?wsid=/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourcegroups/Quick-Starts-MLOps123/workspaces/quick-starts-ws-mlops123\", \"run_id\": \"deep-learning_1582361152_031f08ea\", \"run_properties\": {\"run_id\": \"deep-learning_1582361152_031f08ea\", \"created_utc\": \"2020-02-22T08:46:02.54077Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"08fd9beb-0577-4bf2-a81a-4ac6d033cd5d\", \"azureml.git.repository_uri\": \"https://github.com/microsoft/MCW-ML-Ops\", \"mlflow.source.git.repoURL\": \"https://github.com/microsoft/MCW-ML-Ops\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"104ca17e449baef95d6bc577a2f0603a53775d9a\", \"mlflow.source.git.commit\": \"104ca17e449baef95d6bc577a2f0603a53775d9a\", \"azureml.git.dirty\": \"True\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-02-22T09:02:12.290267Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=%2FjkEiDqPRzCEoOgP0kGO7YKcIzxI7n8uGKI%2FknFxR4E%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/55_azureml-execution-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt?sv=2019-02-02&sr=b&sig=2Ulwcdu2YJYZl1aEdJsWF4Rc4EnJBLAmRWF8F7Bv%2BJA%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/65_job_prep-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt?sv=2019-02-02&sr=b&sig=64%2BUoVrJ2hRXOQd3KITg5oAXW%2FvakgtA7c0sr9a2TOc%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=SVfta30VQoGQYmu8bYbcYDo7EN2fXQhmJtPCHkPNpzI%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"azureml-logs/75_job_post-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/75_job_post-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt?sv=2019-02-02&sr=b&sig=y4BBVJhwc9K2y90s%2BBxvAVDtVVojI0fmo%2B43d7VRZZM%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"azureml-logs/process_info.json\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=uXXxUp%2Fd%2B62OsCo1jaGC83RAcbAKY%2BrqCs7mmE%2BVM4I%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"azureml-logs/process_status.json\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=EP3tBfnWzy3yAxizDceIpiP73Z7wTZhdTgzCatg85io%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"logs/azureml/128_azureml.log\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/logs/azureml/128_azureml.log?sv=2019-02-02&sr=b&sig=dPKa2lRTfqzqBx3IpKuAKfaTlTZVITo452zBq1YHXv4%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=rokhFEqEFBghr1nPmRC%2BeLRqEwGG6xDw1nAPB9Dnpfo%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://quickstastorage957e4064b.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1582361152_031f08ea/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=4rr%2Bhtu7uAJKpucVN1EsJHjbfLpf0PgOd49%2Fb%2FXyMK0%3D&st=2020-02-22T08%3A52%3A20Z&se=2020-02-22T17%3A02%3A20Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/20_image_build_log.txt\"], [\"azureml-logs/55_azureml-execution-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\"], [\"logs/azureml/128_azureml.log\"]], \"run_duration\": \"0:16:09\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020-02-22 09:00:13,320|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2020-02-22 09:00:13,320|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2020-02-22 09:00:13,326|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2020-02-22 09:00:13,326|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-02-22 09:00:13,581|azureml._base_sdk_common.user_agent|DEBUG|Fetching client info from /root/.azureml/clientinfo.json\\n2020-02-22 09:00:13,581|azureml._base_sdk_common.user_agent|DEBUG|Error loading client info: [Errno 2] No such file or directory: '/root/.azureml/clientinfo.json'\\n2020-02-22 09:00:13,952|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f61d0f14158> for run source azureml.scriptrun\\n2020-02-22 09:00:13,953|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-02-22 09:00:13,961|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:13,961|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-02-22 09:00:13,962|azureml.core.authentication|DEBUG|Time to expire 1813548.038013 seconds\\n2020-02-22 09:00:13,962|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,962|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,963|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,963|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,963|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,993|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,993|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,993|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:13,999|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:14,009|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:14,016|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:14,022|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:14,029|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:14,029|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-02-22 09:00:14,030|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-22 09:00:14,030|msrest.http_logger|DEBUG|Request URL: 'https://northeurope.experiments.azureml.net/history/v1.0/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourceGroups/Quick-Starts-MLOps123/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-mlops123/experiments/deep-learning/runs/deep-learning_1582361152_031f08ea'\\n2020-02-22 09:00:14,030|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-02-22 09:00:14,030|msrest.http_logger|DEBUG|Request headers:\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '7ca86293-aa23-4360-af7b-0c6a7ae73188'\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|    'request-id': '7ca86293-aa23-4360-af7b-0c6a7ae73188'\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-buster-sid) msrest/0.6.11 azureml._restclient/core.1.0.85'\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|Request body:\\n2020-02-22 09:00:14,031|msrest.http_logger|DEBUG|None\\n2020-02-22 09:00:14,031|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-22 09:00:14,031|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-22 09:00:14,031|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-22 09:00:14,031|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-22 09:00:14,124|msrest.http_logger|DEBUG|Response status: 200\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|Response headers:\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|    'Date': 'Sat, 22 Feb 2020 09:00:14 GMT'\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-02-22 09:00:14,125|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b'\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '7ca86293-aa23-4360-af7b-0c6a7ae73188'\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|    'x-request-time': '0.076'\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|Response content:\\n2020-02-22 09:00:14,126|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 1,\\n  \\\"rootRunId\\\": \\\"deep-learning_1582361152_031f08ea\\\",\\n  \\\"experimentId\\\": \\\"6ae2b45b-611c-4e6f-bf97-dd6860470fe3\\\",\\n  \\\"createdUtc\\\": \\\"2020-02-22T08:46:02.5407704+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"117829eb-ee64-4518-bc06-695b8c8cf896\\\",\\n    \\\"userPuId\\\": \\\"10033FFF96DE7CED\\\",\\n    \\\"userIdp\\\": null,\\n    \\\"userAltSecId\\\": null,\\n    \\\"userIss\\\": \\\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\\",\\n    \\\"userTenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\",\\n    \\\"userName\\\": \\\"Michal Marusan\\\"\\n  },\\n  \\\"userId\\\": \\\"117829eb-ee64-4518-bc06-695b8c8cf896\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 7,\\n  \\\"runUuid\\\": \\\"3f6ff1ef-270f-402b-92e0-1551f6f1ca98\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"3f6ff1ef-270f-402b-92e0-1551f6f1ca98\\\",\\n  \\\"runId\\\": \\\"deep-learning_1582361152_031f08ea\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-02-22T08:58:14.6843743+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.deep-learning_1582361152_031f08ea\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"08fd9beb-0577-4bf2-a81a-4ac6d033cd5d\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"https://github.com/microsoft/MCW-ML-Ops\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"https://github.com/microsoft/MCW-ML-Ops\\\",\\n    \\\"azureml.git.branch\\\": \\\"master\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"master\\\",\\n    \\\"azureml.git.commit\\\": \\\"104ca17e449baef95d6bc577a2f0603a53775d9a\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"104ca17e449baef95d6bc577a2f0603a53775d9a\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"train.py\\\",\\n  \\\"target\\\": \\\"gpucluster\\\",\\n  \\\"uniqueChildRunComputeTargets\\\": [],\\n  \\\"tags\\\": {},\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://northeurope.experiments.azureml.net/execution/v1.0/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourceGroups/Quick-Starts-MLOps123/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-mlops123/experiments/deep-learning/runId/deep-learning_1582361152_031f08ea/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://northeurope.experiments.azureml.net/execution/v1.0/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourceGroups/Quick-Starts-MLOps123/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-mlops123/experiments/deep-learning/runId/deep-learning_1582361152_031f08ea/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-02-22 09:00:14,132|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-02-22 09:00:14,133|azureml._SubmittedRun#deep-learning_1582361152_031f08ea|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '08fd9beb-0577-4bf2-a81a-4ac6d033cd5d', 'azureml.git.repository_uri': 'https://github.com/microsoft/MCW-ML-Ops', 'mlflow.source.git.repoURL': 'https://github.com/microsoft/MCW-ML-Ops', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '104ca17e449baef95d6bc577a2f0603a53775d9a', 'mlflow.source.git.commit': '104ca17e449baef95d6bc577a2f0603a53775d9a', 'azureml.git.dirty': 'True', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-02-22 09:00:14,133|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-02-22 09:00:14,134|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2020-02-22 09:00:14,134|azureml.WorkerPool|DEBUG|[START]\\n2020-02-22 09:00:14,134|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-02-22 09:00:14,134|azureml.RunStatusContext|DEBUG|[START]\\n2020-02-22 09:00:14,134|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-02-22 09:00:14,134|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-02-22 09:00:14,134|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-02-22 09:00:14,135|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/quick-starts-ws-mlops123/8279625428ce4702a55a3ad61bf295d5/deep-learning_1582361152_031f08ea/mounts/workspaceblobstore/azureml/deep-learning_1582361152_031f08ea\\n2020-02-22 09:00:14,135|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-02-22 09:00:14,135|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/quick-starts-ws-mlops123/8279625428ce4702a55a3ad61bf295d5/deep-learning_1582361152_031f08ea/mounts/workspaceblobstore/azureml/deep-learning_1582361152_031f08ea\\n2020-02-22 09:00:15,955|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,955|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,955|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,955|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,955|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,956|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,956|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northeurope.experiments.azureml.net.\\n2020-02-22 09:00:15,963|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:15,964|azureml._run_impl.run_history_facade|DEBUG|Created a static thread pool for RunHistoryFacade class\\n2020-02-22 09:00:15,969|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:15,976|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:15,982|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:15,988|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:15,989|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-02-22 09:00:15,989|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-22 09:00:15,989|msrest.http_logger|DEBUG|Request URL: 'https://northeurope.experiments.azureml.net/history/v1.0/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourceGroups/Quick-Starts-MLOps123/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-mlops123/experiments/deep-learning/runs/deep-learning_1582361152_031f08ea'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|Request headers:\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '9e7d4ea1-8a52-4657-bf70-f2a469270a86'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|    'request-id': '9e7d4ea1-8a52-4657-bf70-f2a469270a86'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1067-azure-x86_64-with-debian-buster-sid) msrest/0.6.11 azureml._restclient/core.1.0.85'\\n2020-02-22 09:00:15,990|msrest.http_logger|DEBUG|Request body:\\n2020-02-22 09:00:15,991|msrest.http_logger|DEBUG|None\\n2020-02-22 09:00:15,991|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-22 09:00:15,991|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-22 09:00:15,991|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-22 09:00:15,991|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-22 09:00:16,032|msrest.http_logger|DEBUG|Response status: 200\\n2020-02-22 09:00:16,032|msrest.http_logger|DEBUG|Response headers:\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Date': 'Sat, 22 Feb 2020 09:00:16 GMT'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '9e7d4ea1-8a52-4657-bf70-f2a469270a86'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'x-request-time': '0.023'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-02-22 09:00:16,033|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-02-22 09:00:16,034|msrest.http_logger|DEBUG|Response content:\\n2020-02-22 09:00:16,034|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 1,\\n  \\\"rootRunId\\\": \\\"deep-learning_1582361152_031f08ea\\\",\\n  \\\"experimentId\\\": \\\"6ae2b45b-611c-4e6f-bf97-dd6860470fe3\\\",\\n  \\\"createdUtc\\\": \\\"2020-02-22T08:46:02.5407704+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"117829eb-ee64-4518-bc06-695b8c8cf896\\\",\\n    \\\"userPuId\\\": \\\"10033FFF96DE7CED\\\",\\n    \\\"userIdp\\\": null,\\n    \\\"userAltSecId\\\": null,\\n    \\\"userIss\\\": \\\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\\",\\n    \\\"userTenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\",\\n    \\\"userName\\\": \\\"Michal Marusan\\\"\\n  },\\n  \\\"userId\\\": \\\"117829eb-ee64-4518-bc06-695b8c8cf896\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 7,\\n  \\\"runUuid\\\": \\\"3f6ff1ef-270f-402b-92e0-1551f6f1ca98\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"3f6ff1ef-270f-402b-92e0-1551f6f1ca98\\\",\\n  \\\"runId\\\": \\\"deep-learning_1582361152_031f08ea\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-02-22T08:58:14.6843743+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.deep-learning_1582361152_031f08ea\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"08fd9beb-0577-4bf2-a81a-4ac6d033cd5d\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"https://github.com/microsoft/MCW-ML-Ops\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"https://github.com/microsoft/MCW-ML-Ops\\\",\\n    \\\"azureml.git.branch\\\": \\\"master\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"master\\\",\\n    \\\"azureml.git.commit\\\": \\\"104ca17e449baef95d6bc577a2f0603a53775d9a\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"104ca17e449baef95d6bc577a2f0603a53775d9a\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"train.py\\\",\\n  \\\"target\\\": \\\"gpucluster\\\",\\n  \\\"uniqueChildRunComputeTargets\\\": [],\\n  \\\"tags\\\": {},\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://northeurope.experiments.azureml.net/execution/v1.0/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourceGroups/Quick-Starts-MLOps123/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-mlops123/experiments/deep-learning/runId/deep-learning_1582361152_031f08ea/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://northeurope.experiments.azureml.net/execution/v1.0/subscriptions/6ee947fa-0d77-4915-bf68-4a83a8bec2a4/resourceGroups/Quick-Starts-MLOps123/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-mlops123/experiments/deep-learning/runId/deep-learning_1582361152_031f08ea/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-02-22 09:00:16,035|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-02-22 09:00:16,036|azureml._SubmittedRun#deep-learning_1582361152_031f08ea|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '08fd9beb-0577-4bf2-a81a-4ac6d033cd5d', 'azureml.git.repository_uri': 'https://github.com/microsoft/MCW-ML-Ops', 'mlflow.source.git.repoURL': 'https://github.com/microsoft/MCW-ML-Ops', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '104ca17e449baef95d6bc577a2f0603a53775d9a', 'mlflow.source.git.commit': '104ca17e449baef95d6bc577a2f0603a53775d9a', 'azureml.git.dirty': 'True', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-02-22 09:00:16,036|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-02-22 09:00:16,042|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:16,043|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-22 09:00:16,043|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-22 09:00:16,043|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-22 09:00:16,043|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-22 09:00:16,043|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-22 09:00:16,285|msrest.exceptions|DEBUG|Operation returned an invalid status code \\\"Dataset with name 'glove_6B_100d' is not found.\\\"\\n2020-02-22 09:00:30,098|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:30,119|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-22 09:00:30,119|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-22 09:00:30,119|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-22 09:00:30,119|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-22 09:00:30,121|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-22 09:00:43,954|azureml.core.authentication|DEBUG|Time to expire 1813518.045437 seconds\\n2020-02-22 09:00:55,698|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:55,699|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-22 09:00:55,699|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-22 09:00:55,700|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-22 09:00:55,700|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-22 09:00:55,700|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-22 09:00:55,903|msrest.exceptions|DEBUG|Operation returned an invalid status code \\\"Dataset with name 'connected_car_components' is not found.\\\"\\n2020-02-22 09:00:59,982|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-02-22 09:00:59,994|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-02-22 09:00:59,994|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-02-22 09:00:59,994|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-02-22 09:00:59,995|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-02-22 09:00:59,995|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-02-22 09:01:13,993|azureml.core.authentication|DEBUG|Time to expire 1813488.006372 seconds\\n2020-02-22 09:01:43,960|azureml.core.authentication|DEBUG|Time to expire 1813458.039244 seconds\\n2020-02-22 09:01:48,912|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-02-22 09:01:48,913|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/quick-starts-ws-mlops123/8279625428ce4702a55a3ad61bf295d5/deep-learning_1582361152_031f08ea/mounts/workspaceblobstore/azureml/deep-learning_1582361152_031f08ea\\n2020-02-22 09:01:48,913|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /mnt/batch/tasks/shared/LS_root/jobs/quick-starts-ws-mlops123/8279625428ce4702a55a3ad61bf295d5/deep-learning_1582361152_031f08ea/mounts/workspaceblobstore/azureml/deep-learning_1582361152_031f08ea to /mnt/batch/tasks/shared/LS_root/jobs/quick-starts-ws-mlops123/8279625428ce4702a55a3ad61bf295d5/deep-learning_1582361152_031f08ea/mounts/workspaceblobstore/azureml/deep-learning_1582361152_031f08ea\\n2020-02-22 09:01:48,913|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /mnt/batch/tasks/shared/LS_root/jobs/quick-starts-ws-mlops123/8279625428ce4702a55a3ad61bf295d5/deep-learning_1582361152_031f08ea/mounts/workspaceblobstore/azureml/deep-learning_1582361152_031f08ea\\n2020-02-22 09:01:48,913|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-02-22 09:01:48,913|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-02-22 09:01:48,913|azureml._SubmittedRun#deep-learning_1582361152_031f08ea|INFO|complete is not setting status for submitted runs.\\n2020-02-22 09:01:48,914|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-02-22 09:01:48,914|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-02-22 09:01:48,914|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-02-22 09:01:48,914|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-02-22 09:01:48,914|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-02-22 09:01:48,915|azureml.RunStatusContext|DEBUG|[STOP]\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-02-22 09:01:48,915|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-02-22 09:01:48,916|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-02-22 09:01:48,917|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-02-22 09:01:48,917|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-02-22 09:01:48,917|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-02-22 09:01:48,917|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-02-22 09:01:48,917|azureml._SubmittedRun#deep-learning_1582361152_031f08ea.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-02-22 09:01:48,917|azureml.SendRunKillSignal|DEBUG|[STOP]\\n2020-02-22 09:01:48,917|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[START]\\n2020-02-22 09:01:48,917|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[STOP]\\n2020-02-22 09:01:48,917|azureml.WorkerPool|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.72\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Download the model files from the run"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the training script, the Keras model is saved into two files, model.json and model.h5, in the outputs/models folder on the GPU cluster AmlCompute node. Azure ML automatically uploaded anything written in the ./outputs folder into run history file store. Subsequently, we can use the run object to download the model files. They are under the the outputs/model folder in the run history file store, and are downloaded into a local folder named model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for f in run.get_file_names():\n    print(f)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "azureml-logs/20_image_build_log.txt\nazureml-logs/55_azureml-execution-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\nazureml-logs/65_job_prep-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\nazureml-logs/70_driver_log.txt\nazureml-logs/75_job_post-tvmps_3de1cc5e97e63143fd55036e1864e4122a4e82c21b700df4ae4bc7e16ee73a9e_d.txt\nazureml-logs/process_info.json\nazureml-logs/process_status.json\nlogs/azureml/128_azureml.log\nlogs/azureml/job_prep_azureml.log\nlogs/azureml/job_release_azureml.log\noutputs/model/model.h5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a model folder in the current directory\nos.makedirs('./model', exist_ok=True)\n\nfor f in run.get_file_names():\n    if f.startswith('outputs/model'):\n        output_file_path = os.path.join('./model', f.split('/')[-1])\n        print('Downloading from {} to {} ...'.format(f, output_file_path))\n        run.download_file(name=f, output_file_path=output_file_path)\n        print('Download completed.')",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading from outputs/model/model.h5 to ./model/model.h5 ...\nDownload completed.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Restore the model from model.h5 file"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = load_model('./model/model.h5')\nprint(\"Model loaded from disk.\")\nprint(model.summary())",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "WARNING - From /home/nbuser/anaconda3_501/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Model loaded from disk.\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 100)          1000000   \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 10000)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                640064    \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 1,642,177\nTrainable params: 642,177\nNon-trainable params: 1,000,000\n_________________________________________________________________\nNone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Evaluate the model on test data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also evaluate how accurately the model performs against data it has not seen. Run the following cell to load the test data that was not used in either training or evaluating the model. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load the car components labeled data\nprint(\"Loading car components data...\")\n\ncardata_ds = Dataset.get_by_name(workspace=ws, name=cardata_ds_name)\ncar_components_df = cardata_ds.to_pandas_dataframe()\ncomponents = car_components_df[\"text\"].tolist()\nlabels = car_components_df[\"label\"].tolist()\n\nprint(\"Loading car components data completed.\")   \n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(components)\nsequences = tokenizer.texts_to_sequences(components)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=embedding_dim)\n\nlabels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\nindices = np.arange(data.shape[0])      \nnp.random.seed(100)\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\nx_test = data[training_samples + validation_samples:]\ny_test = labels[training_samples + validation_samples:]",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading car components data...\nLoading car components data completed.\nFound 65 unique tokens.\nShape of data tensor: (100000, 100)\nShape of label tensor: (100000,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Run the following cell to see the accuracy on the test set (it is the second number in the array displayed, on a scale from 0 to 1)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('Model evaluation will print the following metrics: ', model.metrics_names)\nevaluation_metrics = model.evaluate(x_test, y_test)\nprint(evaluation_metrics)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model evaluation will print the following metrics:  ['loss', 'acc']\n5000/5000 [==============================] - 1s 264us/step\n[0.7086361988067627, 0.49239999055862427]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Log the evaluation metrics to the to the experiment **Run**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.log(model.metrics_names[0], evaluation_metrics[0], 'Model test data loss')\nrun.log(model.metrics_names[1], evaluation_metrics[1], 'Model test data accuracy')",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Save the run information to json file**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run_info = {}\nrun_info[\"id\"] = run.id\n\nprint(\"Saving run_info.json...\")\nos.makedirs('./outputs', exist_ok=True)\nrun_info_filepath = os.path.join('./outputs', 'run_info.json')\nwith open(run_info_filepath, \"w\") as f:\n    json.dump(run_info, f)",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Saving run_info.json...\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "Deep Learning",
    "notebookId": 2340934485665719
  },
  "nbformat": 4,
  "nbformat_minor": 1
}